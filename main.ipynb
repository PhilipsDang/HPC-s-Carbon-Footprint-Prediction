{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3355d-d4d3-4e4d-9d2d-b033a06819c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfaee4-51ea-4ee0-bbdd-8057aaadfc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066819f2-9738-4ebe-994b-01b4bba89e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the data\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"../data.csv\")\n",
    "\n",
    "# Ensure 'year' is an integer\n",
    "data[\"year\"] = data[\"year\"].astype(int)\n",
    "\n",
    "# Sort the data by year (in case it's not)\n",
    "data = data.sort_values(\"year\").reset_index(drop=True)\n",
    "\n",
    "# Extract the emissions data and scale it between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "emissions = scaler.fit_transform(\n",
    "    data[\n",
    "        [\n",
    "            \"Coal_Emissions_Global\",\n",
    "            \"NaturalGas_Emissions_Global\",\n",
    "            \"Petroleum_Emissions_Global\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165a80c-1bad-49c9-8d06-338534af6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a PyTorch dataset and dataloader\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index : index + self.seq_length]\n",
    "        y = self.data[index + self.seq_length]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Define the sequence length\n",
    "SEQ_LENGTH = 5  # Number of previous years to consider\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TimeSeriesDataset(emissions, SEQ_LENGTH)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f0aec-bb43-42c2-a996-5dcb761c0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Transformer model\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create constant 'pe' matrix with values dependent on\n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input tensor\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_projection = nn.Linear(num_features, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "        self.decoder = nn.Linear(d_model, num_features)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, seq_length, num_features]\n",
    "        src = self.input_projection(src) * np.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        # Transformer expects input shape: [seq_length, batch_size, d_model]\n",
    "        src = src.permute(1, 0, 2)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Take the output from the last time step\n",
    "        output = output[-1, :, :]\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = TimeSeriesTransformer(num_features=emissions.shape[1]).to(DEVICE)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7f04b-f935-4fb1-bb71-f8feec579ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{EPOCHS}, \"\n",
    "        f\"Training Loss: {train_loss:.4f}, \"\n",
    "        f\"Validation Loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422e312-d99a-4546-8b6e-9782f2c4e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predict future emissions until 2030\n",
    "\n",
    "# Prepare to predict future emissions\n",
    "model.eval()\n",
    "\n",
    "# Number of years to predict\n",
    "last_year = data[\"year\"].iloc[-1]\n",
    "predict_years = np.arange(last_year + 1, 2031)\n",
    "num_predictions = len(predict_years)\n",
    "\n",
    "# Initialize the input sequence with the last known data\n",
    "input_seq = (\n",
    "    torch.tensor(emissions[-SEQ_LENGTH:], dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    ")\n",
    "\n",
    "# List to store predictions\n",
    "predictions = []\n",
    "\n",
    "for _ in range(num_predictions):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "    predictions.append(output.cpu().numpy())\n",
    "    # Prepare the input for the next prediction\n",
    "    next_input = output.unsqueeze(0)\n",
    "    input_seq = torch.cat((input_seq[:, 1:, :], next_input), dim=1)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "predictions = np.array(predictions).squeeze(\n",
    "    axis=1\n",
    ")  # Shape: (num_predictions, num_features)\n",
    "predictions = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5695a-e3af-41a0-b95e-b2520ceaae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare and save the combined data\n",
    "\n",
    "# Create a DataFrame for predicted data\n",
    "future_data = pd.DataFrame(\n",
    "    predictions,\n",
    "    columns=[\n",
    "        \"Coal_Emissions_Global\",\n",
    "        \"NaturalGas_Emissions_Global\",\n",
    "        \"Petroleum_Emissions_Global\",\n",
    "    ],\n",
    ")\n",
    "future_data[\"year\"] = predict_years\n",
    "\n",
    "# Combine historical and predicted data\n",
    "historical_data = data.copy()\n",
    "combined_data = pd.concat([historical_data, future_data], ignore_index=True)\n",
    "\n",
    "# Compute total emissions\n",
    "combined_data[\"Total_Emissions\"] = (\n",
    "    combined_data[\"Coal_Emissions_Global\"]\n",
    "    + combined_data[\"NaturalGas_Emissions_Global\"]\n",
    "    + combined_data[\"Petroleum_Emissions_Global\"]\n",
    ")\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "combined_data.to_csv(\"emissions_predictions.csv\", index=False)\n",
    "print('Combined data saved to \"emissions_predictions.csv\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ea381-74a9-4d63-80b9-f0f1405129b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Plot and save the data\n",
    "# ------------------------------\n",
    "\n",
    "# Plot total emissions over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    combined_data[\"year\"],\n",
    "    combined_data[\"Total_Emissions\"],\n",
    "    label=\"Total Emissions\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.axvline(\n",
    "    x=last_year,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Prediction Start (Year {})\".format(last_year + 1),\n",
    ")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Emissions\")\n",
    "plt.title(\"Historical and Predicted Total Emissions (Global)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"total_emissions.png\")\n",
    "plt.show()\n",
    "print('Plot saved as \"total_emissions.png\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83a9f6-6a50-4942-a0e7-f3c259fdc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Display total emissions for predicted years\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Calculate total predicted emissions from the predicted years\n",
    "predicted_total_emissions = (\n",
    "    future_data[\"Coal_Emissions_Global\"].sum()\n",
    "    + future_data[\"NaturalGas_Emissions_Global\"].sum()\n",
    "    + future_data[\"Petroleum_Emissions_Global\"].sum()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Total predicted emissions from {predict_years[0]} to {predict_years[-1]}: \"\n",
    "    f\"{predicted_total_emissions:.2f}\"\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated labels and data\n",
    "labels = [\n",
    "    \"Coal_Emissions_Global\",\n",
    "    \"NaturalGas_Emissions_Global\",\n",
    "    \"Petroleum_Emissions_Global\",\n",
    "]\n",
    "sizes = [\n",
    "    397.8934631347656,\n",
    "    264.2558288574219,\n",
    "    641.5406494140625,\n",
    "]\n",
    "\n",
    "# Plot configuration\n",
    "colors = [\"#ff9999\", \"#66b3ff\", \"#99ff99\"]  # Pleasant colors for each segment\n",
    "explode = (0.0, 0.0, 0.0)  # Only \"explode\" the first slice\n",
    "\n",
    "# Create the pie chart\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(\n",
    "    sizes,\n",
    "    explode=explode,\n",
    "    labels=labels,\n",
    "    colors=colors,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    ")\n",
    "ax.axis(\"equal\")  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# Set the background color of the figure to be transparent\n",
    "fig.patch.set_alpha(0.0)\n",
    "ax.patch.set_alpha(0.0)\n",
    "\n",
    "# Save the plot with a transparent background\n",
    "plt.savefig(\n",
    "    \"pie_chart_transparent_background.png\", bbox_inches=\"tight\", transparent=True\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
